### <center>**字符粒度策略环境：形式化建模与离散SAC**</center>

---

> 定位：面向生产的**字符级 POMDP**与**离散最大熵 SAC**训练方案，内置**双缓冲 + BC**并轨、**Top-K 期望近似**、**硬掩码合规**与**在线可观测 KPI**。

---

### 0. 符号与范围

* 字表（含特殊符号）$\Sigma$，合法动作集 $\mathcal{A}(s)\subseteq\Sigma$。
* 折扣 $\gamma\in(0,1)$，温度目标系数 $\kappa\in[0.7,1.2]$。
* Twin-Q：$Q_{\phi_1},Q_{\phi_2}$，目标网络 $Q'_{\bar\phi_1},Q'_{\bar\phi_2}$。
* 策略 $\pi_\theta(a\mid o)$，对数温度参数化 $\tilde\alpha=\log\alpha\Rightarrow \alpha=e^{\tilde\alpha}$。
* 回放：$\mathcal{D}_{agent}$、$\mathcal{D}_{demo}$；混采比例 $\rho\in(0,1)$（默认 $\rho=0.75$）。

---

### 1. 任务建模（无泄漏 POMDP）

定义 POMDP 七元组 $(\mathcal{S},\mathcal{A},\mathcal{T},\mathcal{R},\Omega,\mathcal{O},\gamma)$。

* **状态 $\mathcal{S}$**：包含**代理已生成的最近 $m$ 个字符**的滑窗 $h_{t-1}\in\Sigma^{m}$ 与可选的轻量上下文摘要向量 $u_{t-1}\in\mathbb{R}^{d_c}$（由 RNN/Conv/Transformer 汇总，工程默认 GRU）。
* **动作 $\mathcal{A}=\Sigma$**：单字符决策。
* **观测 $\Omega$** 与**观测函数 $\mathcal{O}$**：

  $$
  o_t=\mathcal{O}(s_t)=\mathrm{Enc}(h_{t-1})\ \oplus\ u_{t-1}\quad(\text{不包含任何真值或未来片段})
  $$

  **严禁**把参考对 $\tau_t$ 或未来目标暴露给策略侧（泄漏一票否决）。
* **转移 $\mathcal{T}$**（确定性）：

  $$
  h_t=\mathrm{shift\_append}(h_{t-1}, a_t),\quad u_t=\mathrm{GRU}(u_{t-1},\mathrm{Emb}(a_t))
  $$

  若触发教师覆盖：$(h_t,u_t)\leftarrow(g_t,\ \mathrm{GRU}(u_{t-1},\mathrm{Emb}(g_t^{(1)})))$。
* **奖励 $\mathcal{R}$**：见 §4。
* **终止**：到达预设步长 $T$ 或触发非法硬约束早停。

---

### 2. 合规与动作空间（硬掩码）

* 定义合规函数 $\mathrm{Compliance}(h_{t-1},a)\in\{0,1\}$。
* 合法集 $\mathcal{A}(s_t)=\{a\in\Sigma\mid \mathrm{Compliance}(h_{t-1},a)=1\}\cup\{\langle\mathrm{eos}\rangle\}$。
* **硬掩码**到 logits：

  $$
  \tilde z_i=\begin{cases}
  z_i & i\in\mathcal{A}(s_t)\\
  -\infty & \text{否则}
  \end{cases},\quad 
  \pi_\theta(a\mid o)=\mathrm{softmax}(\tilde z)
  $$
* 合法集自适应熵目标：$\bar{\mathcal H}(s)=\kappa\cdot \log(|\mathcal{A}(s)|)$（nats）。

---

### 3. 模型结构（单步分类头，对齐环境）

* **Encoder**：字符嵌入 $E:\Sigma\to\mathbb{R}^{d}$，GRU/Conv 生成 $c_t\in\mathbb{R}^{d}$。
* **策略头**：$z=\mathrm{MLP}_\theta(c_t)\in\mathbb{R}^{|\Sigma|}$；上硬掩码得 $\tilde z$，再 softmax。
* **Twin-Q**：$Q_{\phi_i}(o,a)=\mathrm{MLP}_{\phi_i}([c_t;\ \mathrm{onehot}(a)])$。
* **目标网络**：$\bar\phi_i$ 指数滑动更新。

> 取消 Seq2Seq 解码器；环境只需一个字符动作，训练语义与环境一致，算力更聚焦。

---

### 4. 奖励设计（拉长信用分配）

令参考文本的 n-gram 序列为 $\{g^{(n)}_t\}$。在每步把已生成序列与参考做对齐评估：

1. **n-gram 覆盖**（滑窗 $W$）

$$
\mathrm{cov}_t=\frac{1}{|S_t|}\sum_{x\in S_t}\mathbf{1}[x\in \mathcal{G}_t],\quad S_t=\text{Agent }n\text{-gram}(t,W),\ \mathcal{G}_t=\text{Ref }n\text{-gram}(t,W)
$$

2. **对比相似（InfoNCE）**
   构造正样 $x^+=g^{(n)}_t$，负样 $x^- \in \mathcal{N}_t$（同域难例）。

$$
\mathrm{nce}_t=\log\frac{\exp(\langle f(S_t),f(x^+)\rangle/\tau)}{\exp(\langle f(S_t),f(x^+)\rangle/\tau)+\sum_{x^-\in\mathcal{N}_t}\exp(\langle f(S_t),f(x^-)\rangle/\tau)}
$$

3. **洁净/非法罚**

$$
\mathrm{ill}_t=\mathbf{1}[a_t\notin\mathcal{A}(s_t)],\quad \mathrm{garble}_t=\mathsf{Garble}(a_t)
$$

4. **步级奖励**（去掉 $B_t,\Delta\Phi_t$ 等恒零项）

$$
r_t=\lambda_{\mathrm{cov}}\cdot \mathcal{N}_\gamma(\mathrm{cov}_t)+\lambda_{\mathrm{nce}}\cdot \mathrm{nce}_t-\lambda_{\mathrm{gar}}\cdot \mathrm{garble}_t-\lambda_{\mathrm{ill}}\cdot \mathrm{ill}_t
$$

* **非法动作策略**：$\mathrm{ill}_t=1 \Rightarrow r_t=-\lambda_{\mathrm{ill}}$，并可选 `done=True`（硬边界）。
* 建议：$n\in\{3,4\}$、$W=64$、$\tau=0.07$。

---

### 5. 离散最大熵 SAC（期望备份 + Top-K）

**软值函数**（使用目标网络与当前策略）

$$
V_{\text{soft}}(s')=\sum_{a'\in\mathcal{A}(s')} \pi_\theta(a'\mid o')\Big[\min_{i=1,2}Q'_{\bar\phi_i}(o',a')-\alpha\log\pi_\theta(a'\mid o')\Big]
$$

**Top-K 近似**（降耗）

* 取 $\mathcal{K}(s')=\mathrm{TopK}(\tilde z'(s'),K)$，定义 $\pi_K\propto \pi\cdot \mathbf{1}[a\in\mathcal{K}]$。

$$
\hat V_{\text{soft}}(s')=\sum_{a'\in\mathcal{K}(s')} \pi_K(a'\mid o')\Big[\min_i Q'_{\bar\phi_i}(o',a')-\alpha\log\pi_K(a'\mid o')\Big]
$$

**Bellman 目标与损失**

$$
y_t=r_t+\gamma\, \hat V_{\text{soft}}(s_{t+1})
$$

$$
\mathcal{L}_{Q}=\mathbb{E}_{(o,a,r,o')\sim\mathcal{B}}\sum_{i=1}^{2}\big(Q_{\phi_i}(o,a)-y_t\big)^2
$$

**策略目标（期望式）**

$$
\mathcal{L}_\pi=\mathbb{E}_{o\sim\mathcal{B}}\ \sum_{a\in\mathcal{A}(s)}\pi_\theta(a\mid o)\Big[\alpha\log\pi_\theta(a\mid o)-\min_i Q_{\phi_i}(o,a)\Big]
$$

**温度自适应（对 $\tilde\alpha=\log\alpha$）**

$$
\tilde\alpha \leftarrow \tilde\alpha+\eta_\alpha\cdot \mathbb{E}_{o,a\sim\pi}\big[-\log\pi_\theta(a\mid o)-\bar{\mathcal H}(s)\big],\quad \bar{\mathcal H}(s)=\kappa\log|\mathcal{A}(s)|
$$

**目标网络软更新**

$$
\bar\phi_i \leftarrow \tau\phi_i+(1-\tau)\bar\phi_i
$$

---

### 6. 教师并轨（双缓冲 + BC + DAgger）

* **双缓冲**：教师步存入 $\mathcal{D}_{demo}$（标注 `is_demo=1`），代理步存入 $\mathcal{D}_{agent}$。
* **混采**：每次训练批 $\mathcal{B}=\mathcal{B}_{agent}\cup \mathcal{B}_{demo}$，$|\mathcal{B}_{agent}|:\ |\mathcal{B}_{demo}|=\rho:(1-\rho)$。
* **BC 辅助项**（仅对 `is_demo=1`）

$$
\mathcal{L}_{BC}=\lambda_{BC}\cdot \mathbb{E}_{(o,a^*)\in\mathcal{B}_{demo}}\big[-\log \pi_\theta(a^*\mid o)\big]
$$

* **总策略损失**：$\mathcal{L}_{\pi}^{tot}=\mathcal{L}_{\pi}+\mathcal{L}_{BC}$。
* **DAgger 调度**：`teacher_ratio` 从 1.0 → 0.1 线性退火，步长对齐训练轮次。

---

### 7. 最小可用训练循环（伪代码）

```python
# 初始化
init_policy(theta); init_q(phi1, phi2); init_target_q(bar_phi1, bar_phi2 <- phi);
alpha = exp(log_alpha); replay_agent = RB(); replay_demo = RB()

for episode in range(E):
    s = env.reset()
    for t in range(T):
        # 合法集硬掩码、采样
        logits = policy_logits(theta, o(s)); logits[illegal_mask(s)] = -inf
        a ~ Categorical(softmax(logits))
        s_next, r, done, info = env.step(a)

        # 写缓冲：教师步写 demo（带 is_demo=1），普通步写 agent
        if info["is_teacher"]:
            replay_demo.add(o(s), info["teacher_action"], r, o(s_next), done, is_demo=1)
        else:
            replay_agent.add(o(s), a, r, o(s_next), done, is_demo=0)

        # 训练若可
        if ready():
            B_agent = replay_agent.sample(batch_size * rho)
            B_demo  = replay_demo.sample(batch_size * (1 - rho))
            B = merge(B_agent, B_demo)

            # --- Critic ---
            with no_grad():
                logits_next = policy_logits(theta, o_next(B)); mask_next = illegal_mask(s_next(B))
                logits_next[mask_next] = -inf
                # Top-K 近似
                K_idx = topk(logits_next, K)
                pi_K = softmax(logits_next.mask(~K_idx, -inf))
                q1_t = target_q1(bar_phi1, o_next(B), A_K)
                q2_t = target_q2(bar_phi2, o_next(B), A_K)
                v_soft = sum_over_AK(pi_K * (min(q1_t, q2_t) - alpha * log(pi_K)))
                y = r(B) + gamma * v_soft

            L_Q = mse(q1(phi1, o(B), a(B)), y) + mse(q2(phi2, o(B), a(B)), y)
            update(phi1, phi2) to minimize L_Q

            # --- Policy (+ BC) ---
            logits_curr = policy_logits(theta, o(B)); logits_curr[illegal_mask(s(B))] = -inf
            pi = softmax(logits_curr)
            qmin = min(q1(phi1, o(B), A_all), q2(phi2, o(B), A_all))
            L_pi = mean(sum(pi * (alpha * log(pi) - qmin), axis=actions))

            L_BC = lambda_BC * CE(pi, a_star(B_demo_only))
            update(theta) to minimize (L_pi + L_BC)

            # --- Temperature ---
            with no_grad():
                H_target = kappa * log(legal_count(s(B)))
                entropy = -sum(pi * log(pi), axis=actions)
            log_alpha += eta_alpha * mean(entropy - H_target)
            alpha = exp(log_alpha)

            # --- Target update ---
            bar_phi1 = tau * phi1 + (1 - tau) * bar_phi1
            bar_phi2 = tau * phi2 + (1 - tau) * bar_phi2

        if done: break
        s = s_next
```

---

### 8. 潜能塑形不变性

若使用任意有界潜能 $\Phi:\mathcal{S}\to\mathbb{R}$ 进行
$r'_t=r_t+\gamma\Phi(s_{t+1})-\Phi(s_t)$，在 $\gamma<1$ 下
$\arg\max_\pi \mathbb{E}\sum_t \gamma^t r'_t=\arg\max_\pi \mathbb{E}\sum_t \gamma^t r_t$。

---

### 9. 默认参数

* 模型：$d=128$, GRU hidden $=256$。
* 训练：batch $=2048$，lr$_\pi$=3e-4，lr$_Q$=3e-4，lr$_\alpha$=1e-4，$\tau=0.005$。
* SAC：$\gamma=0.997$，Top-K $=64$。
* 熵：$\kappa=0.9$。
* 奖励：$\lambda_{\mathrm{cov}}=1.0$，$\lambda_{\mathrm{nce}}=0.5$，$\lambda_{\mathrm{gar}}=0.1$，$\lambda_{\mathrm{ill}}=2.0$。
* 数据：$\rho=0.75$，$\lambda_{BC}=0.1$，DAgger 线性退火 200k 步到 20k 步。
* 训练安全：梯度裁剪 0.5，mask 在 logit 层；数值溢出监控 `nan/inf` 钩子。

---

### 10. 质量控制（Go-Live Gate · KPI/SLI）

* **稳定性**：三次独立跑，critic 损失无发散；$\alpha$ 收敛入 $[10^{-3}, 1]$。
* **泛化**（域外章节）：Top-1/Top-3 字符命中率、4-gram 覆盖 ≥ 基线 +10pp。
* **合规**：非法字符比 < 0.1%，脏尾=0。
* **长程一致性**：段落级一致性分 ≥ 泄漏基线 +8pp。
* **消融**：去 BC / 去 Top-K / 固定 $\alpha$ 任一项，指标下降 ≥5pp，效用被证实。

---

### 11. 运维与监控

* 训练面板：$\mathbb{E}[H(s)]$、$\alpha$、非法率、$\mathrm{cov}$/$\mathrm{nce}$ 轨迹、Top-K 覆盖率。
* 离线评测：BLEU-n、chrF、人工洁净度审计抽样。
* 在线保护：硬掩码热更、黑名单字符零时延下发；越权即刻早停。

---

### 12. 兼容性说明

* **观测泄漏**已彻底移除：参考对/未来信息只用于奖励与评测。
* **Actor/Env 一致**：去序列解码，单步分类头。
* **离散 SAC 严格化**：期望式备份 + 目标网络 + Top-K 近似。
* **教师并轨**：双缓冲 + BC + DAgger，避免价值污染与分布移位。
* **奖励短视修复**：n-gram + InfoNCE 拉长信用分配；非法强惩 + 早停确立硬边界。

---

### 13. 风险与缓释

* **Top-K 近似偏差**：监控 Top-K 覆盖率 <95% 时提升 K 或做分层采样。
* **合规器过严**：若合法集过小（$<3$），自动放宽为 $\{\text{字母/数字/空白},\langle eos\rangle\}$。
* **教师依赖**：DAgger 退火不达标时，冻结 BC 权重再退火一次（双阶段）。
