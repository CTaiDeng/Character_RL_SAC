# 输入输出数据方案

## 输入描述
- 输入文件 $X = \{x_1, x_2, \dots, x_n\}$ 来源于 `data/sample_article.txt`。
- 每个样本 $x_i$ 为按句切分的文本片段，满足 $|x_i| \leq 200$。
- 预处理函数 $f_{\text{clean}}: X \rightarrow X'$ 去除前后空白、保留原有语义符号。

## 输出描述
- 输出集合 $Y = [y_1, y_2, \dots, y_m]$ 存储于 `data/chinese_name_frequency_word.json`。
- $y_k$ 可以是中文命名实体或混合字母串（如 `P450`），保持去重且按频次降序排序。
- 采用 JSON 数组编码，元素满足字符集约束 $y_k \in (\mathbb{C} \cup \mathbb{A})^+$。
- 构造字符二元参考集合 $B = \{ b \mid b \in \mathcal{D}, |b|=2 \}$，其中 $\mathcal{D}$ 为 `data/chinese_frequency_word.json` 与 `data/chinese_name_frequency_word.json` 的并集。

## 数据流程伪代码
```pseudo
function TransformIO(text_path, json_path):
    raw_text <- read(text_path)
    segments <- chunk(raw_text, max_len=200)
    entities <- []
    for batch in batch_split(segments, size=16):
        ner_batch <- ltp.pipeline(batch, tasks=["cws", "ner"])
        entities.append(filter_tags(ner_batch))
    ascii_tokens <- regex_extract(raw_text)
    bigram_vocab <- load_bigrams(["data/chinese_frequency_word.json", "data/chinese_name_frequency_word.json"])
    merged <- normalize(entities, ascii_tokens)
    sorted_list <- sort_by_frequency(merged)
    register_character_reward(bigram_vocab)
    write_json(json_path, sorted_list)
```
